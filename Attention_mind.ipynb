{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mindspore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "t4ys8IeT0Jzc",
        "outputId": "f6232be5-f61a-49a8-e73d-4ed4bf6e5d3d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mindspore\n",
            "  Downloading mindspore-1.9.0-cp38-cp38-manylinux1_x86_64.whl (158.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 158.7 MB 4.2 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from mindspore) (7.1.2)\n",
            "Collecting asttokens>=2.0.4\n",
            "  Downloading asttokens-2.2.1-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.8/dist-packages (from mindspore) (1.21.6)\n",
            "Requirement already satisfied: protobuf>=3.13.0 in /usr/local/lib/python3.8/dist-packages (from mindspore) (3.19.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from mindspore) (21.3)\n",
            "Requirement already satisfied: astunparse>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from mindspore) (1.6.3)\n",
            "Requirement already satisfied: scipy>=1.5.2 in /usr/local/lib/python3.8/dist-packages (from mindspore) (1.7.3)\n",
            "Collecting psutil>=5.6.1\n",
            "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 65.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from asttokens>=2.0.4->mindspore) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.3->mindspore) (0.38.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->mindspore) (3.0.9)\n",
            "Installing collected packages: psutil, asttokens, mindspore\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "Successfully installed asttokens-2.2.1 mindspore-1.9.0 psutil-5.9.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AbDkKJ1Kz9Yo"
      },
      "outputs": [],
      "source": [
        "import mindspore as ms\n",
        "import mindspore.nn as nn\n",
        "import numpy as np\n",
        "from mindspore.ops import operations as P\n",
        "import mindspore.numpy as mnp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImgPatches(nn.Cell):\n",
        "    def __init__(self, in_ch=3, embed_dim=768, patch_size=16):\n",
        "        super().__init__()\n",
        "        self.patch_embed = nn.Conv2d(in_ch,embed_dim, kernel_size = patch_size, stride = patch_size)#...\n",
        "        \n",
        "    def construct(self, img):\n",
        "        patches = self.patch_embed(img)\n",
        "        x = patches.shape\n",
        "        x = patches.view(x[0],-1,x[1])\n",
        "        return x"
      ],
      "metadata": {
        "id": "_FxTnMHA0kl2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Cell):\n",
        "    def __init__(self, d_h):\n",
        "        super().__init__()\n",
        "        self.d_h = d_h\n",
        "        self.Qu = ms.Parameter(ms.Tensor(np.random.randn(d_h,d_h), ms.float32),name='Q')\n",
        "        self.Ke = ms.Parameter(ms.Tensor(np.random.randn(d_h,d_h), ms.float32),name='K')\n",
        "        self.Va = ms.Parameter(ms.Tensor(np.random.randn(d_h,d_h), ms.float32),name='V')\n",
        "        \n",
        "    def construct(self,x):\n",
        "        batch_s,n,emb_d = x.shape[0],x.shape[1],x.shape[2]\n",
        "        Q = mnp.matmul(x, self.Qu.expand_as(ms.Tensor(np.random.randn(batch_s,self.d_h,self.d_h), ms.float32)))  #[batch_s,n,dh]\n",
        "        K = mnp.matmul(x, self.Ke.expand_as(ms.Tensor(np.random.randn(batch_s,self.d_h,self.d_h), ms.float32)))  #[batch_s,n,dh]\n",
        "        V = mnp.matmul(x, self.Va.expand_as(ms.Tensor(np.random.randn(batch_s,self.d_h,self.d_h), ms.float32)))   #[batch_s,n,dh]\n",
        "\n",
        "        A = mnp.matmul(Q, mnp.transpose(K,[0,2,1]) * 1./(self.d_h**0.5))\n",
        "        SA= mnp.matmul(A,V)\n",
        "        return SA"
      ],
      "metadata": {
        "id": "01OKkZ4GqYy-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Cell):\n",
        "    def __init__(self, dim, num_heads=8, attn_dropout=0.01, proj_dropout=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        #self.scale = 1./dim**0.5\n",
        "        #self.qkv = #...\n",
        "        self.heads = nn.CellList([Head(dim//num_heads) for i in range(num_heads)])\n",
        "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
        "        #self.out = #...\n",
        "\n",
        "    def construct(self, x):\n",
        "        x = x.view(x.shape[0], x.shape[1], self.num_heads, -1)\n",
        "        SA = []\n",
        "        for i in range(self.num_heads):\n",
        "             Sa = nn.Softmax(axis = 2)(self.heads[i](x[:,:,i,:]))\n",
        "             SA.append(Sa)                 \n",
        "        result = mnp.concatenate(SA, axis = 2) #[b, n, dim_embed]\n",
        "        return result"
      ],
      "metadata": {
        "id": "dtri7LYS30pI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Cell):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None,\n",
        "                 dropout=0.01):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_features = hidden_features\n",
        "        if hidden_features != None:\n",
        "            self.linear1 = nn.Dense(in_features, hidden_features)\n",
        "            self.linear2 = nn.Dense(hidden_features, out_features)\n",
        "        else:\n",
        "            self.linear1 = nn.Dense(in_features, out_features)\n",
        "        self.GeLU    = nn.GELU()    \n",
        "        self.drop    = nn.Dropout(dropout)\n",
        "\n",
        "        #...\n",
        "\n",
        "    def construct(self, x):\n",
        "        if self.hidden_features != None:\n",
        "            x = self.linear1(x)\n",
        "            x = self.GeLU(x)\n",
        "            x = self.drop(x)\n",
        "            x = self.linear2(x)\n",
        "        else:\n",
        "            x = self.linear1(x)\n",
        "        #...\n",
        "        return x"
      ],
      "metadata": {
        "id": "pQL2FZjEHy_m"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Cell):\n",
        "    def __init__(self, dim, n_patches, num_heads=8 ,mlp_ratio=4, drop_rate=0.):\n",
        "        super().__init__()\n",
        "        self.l_norm1    = nn.LayerNorm([n_patches,dim],begin_norm_axis=1,begin_params_axis=1)\n",
        "        self.Attention  = Attention(dim = dim, num_heads = num_heads)\n",
        "        self.l_norm2    = nn.LayerNorm([n_patches,dim],begin_params_axis=1,begin_norm_axis=1)\n",
        "        self.MLP        = ms.Parameter(ms.Tensor(np.random.randn(dim,dim), ms.float32))    \n",
        "        #...\n",
        "\n",
        "    def construct(self, x):\n",
        "\n",
        "        x1 = self.l_norm1(x)\n",
        "        x2 = self.Attention(x1)\n",
        "\n",
        "        x = x + x2\n",
        "        x = mnp.matmul(self.l_norm2(x), self.MLP)\n",
        "        return x"
      ],
      "metadata": {
        "id": "5LqsA6ImH9HS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Cell):\n",
        "    def __init__(self, depth, dim, n_patches, num_heads=8, mlp_ratio=4, drop_rate=0.01):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.CellList([\n",
        "            Block(dim,n_patches, num_heads, mlp_ratio, drop_rate)\n",
        "            for i in range(depth)])\n",
        "\n",
        "    def construct(self, x):\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "sxm4kXFXTKMH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Cell):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_ch=3, num_classes=10,\n",
        "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n",
        "                 drop_rate=0.01):\n",
        "        super().__init__()\n",
        "        self.n_patches = (img_size//patch_size)**2 \n",
        "\n",
        "        self.learn_tok   = ms.Parameter(ms.Tensor(np.random.randn(1,embed_dim), ms.float32))    \n",
        "        self.pos_enc     = ms.Tensor([[pos/(10000**(2*i/embed_dim)) for i in range(embed_dim)] for pos in range(self.n_patches+1)], ms.float32)\n",
        "\n",
        "        self.get_patches = ImgPatches(in_ch,embed_dim,patch_size)\n",
        "\n",
        "        self.drop        = nn.Dropout(drop_rate)\n",
        "        self.transformer = Transformer(depth = depth,\n",
        "                                       dim = embed_dim,\n",
        "                                       n_patches= self.n_patches + 1,\n",
        "                                       num_heads=num_heads,\n",
        "                                       mlp_ratio=mlp_ratio, \n",
        "                                       drop_rate=drop_rate)\n",
        "        \n",
        "        self.late_fuse   = LateFusionClass(in_chan = self.n_patches+1, out_chan = self.n_patches+1)\n",
        "\n",
        "        self.MLP         = MLP(in_features = 8*embed_dim,\n",
        "                               out_features = num_classes)\n",
        "\n",
        "        #...\n",
        "\n",
        "    def construct(self, x):\n",
        "        #print(type(x),'here23')\n",
        "        x   = self.get_patches(x)\n",
        "        b,n,emb = x.shape\n",
        "        batch_token = self.learn_tok.expand_as(ms.Tensor(np.random.randn(b,1,emb), ms.float32))\n",
        "        x_  = mnp.concatenate([batch_token, x],axis = 1)\n",
        "        batch_pos_enc = self.pos_enc.expand_as(ms.Tensor(np.random.randn(b,self.n_patches+1,emb), ms.float32))\n",
        "\n",
        "\n",
        "        x_ = x_ + batch_pos_enc\n",
        "        x = self.drop(x_)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "        x = self.late_fuse(x)\n",
        "\n",
        "        x = x[:,0,:]\n",
        "        x = self.MLP(x)\n",
        "        x = nn.Softmax(axis = -1)(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "qz56xnN3gQ4M"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LateFusionClass(nn.Cell):\n",
        "     def __init__(self, in_chan, out_chan):\n",
        "         super().__init__()\n",
        "         self.res = nn.Conv1dTranspose(in_chan,out_chan,kernel_size = 8,stride = 8) \n",
        "     def construct(self, x):\n",
        "         x = self.res(x)\n",
        "         return x\n",
        "         "
      ],
      "metadata": {
        "id": "GymWFVd8Lqzb"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChanWiseClass(nn.Cell):\n",
        "    def __init__(self,in_chan = 32,num_classes=10):\n",
        "        super().__init__()\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.cll =  nn.Dense(in_chan,num_classes)\n",
        "    def construct(self, x):\n",
        "        x = mnp.transpose(x,[0,2,1])\n",
        "        x = self.pool(x)\n",
        "        bs  = x.shape[0]\n",
        "        x = x.view(bs,-1)\n",
        "        x = self.cll(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ZfAQQwV7AMA4"
      },
      "execution_count": 82,
      "outputs": []
    }
  ]
}