{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mindspore"
      ],
      "metadata": {
        "id": "t4ys8IeT0Jzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "AbDkKJ1Kz9Yo"
      },
      "outputs": [],
      "source": [
        "import mindspore as ms\n",
        "import mindspore.nn as nn\n",
        "import numpy as np\n",
        "from mindspore.ops import operations as P\n",
        "import mindspore.numpy as mnp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImgPatches(nn.Cell):\n",
        "    def __init__(self, in_ch=3, embed_dim=768, patch_size=16):\n",
        "        super().__init__()\n",
        "        self.patch_embed = nn.Conv2d(in_ch,embed_dim, kernel_size = patch_size, stride = patch_size)#...\n",
        "        \n",
        "    def construct(self, img):\n",
        "        patches = self.patch_embed(img)\n",
        "        x = patches.shape\n",
        "        x = patches.view(x[0],-1,x[1])\n",
        "        return x"
      ],
      "metadata": {
        "id": "_FxTnMHA0kl2"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Cell):\n",
        "    def __init__(self, d_h):\n",
        "        super().__init__()\n",
        "        self.d_h = d_h\n",
        "        self.Qu = ms.Parameter(ms.Tensor(np.random.randn(d_h,d_h), ms.float32),name='Q')\n",
        "        self.Ke = ms.Parameter(ms.Tensor(np.random.randn(d_h,d_h), ms.float32),name='K')\n",
        "        self.Va = ms.Parameter(ms.Tensor(np.random.randn(d_h,d_h), ms.float32),name='V')\n",
        "        \n",
        "    def construct(self,x):\n",
        "        batch_s,n,emb_d = x.shape[0],x.shape[1],x.shape[2]\n",
        "        Q = mnp.matmul(x, self.Qu.expand_as(ms.Tensor(np.random.randn(batch_s,self.d_h,self.d_h), ms.float32)))  #[batch_s,n,dh]\n",
        "        K = mnp.matmul(x, self.Ke.expand_as(ms.Tensor(np.random.randn(batch_s,self.d_h,self.d_h), ms.float32)))  #[batch_s,n,dh]\n",
        "        V = mnp.matmul(x, self.Va.expand_as(ms.Tensor(np.random.randn(batch_s,self.d_h,self.d_h), ms.float32)))   #[batch_s,n,dh]\n",
        "\n",
        "        A = mnp.matmul(Q, mnp.transpose(K,[0,2,1]) * 1./(self.d_h**0.5))\n",
        "        SA= mnp.matmul(A,V)\n",
        "        return SA"
      ],
      "metadata": {
        "id": "01OKkZ4GqYy-"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Cell):\n",
        "    def __init__(self, dim, num_heads=8, attn_dropout=0.01, proj_dropout=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        #self.scale = 1./dim**0.5\n",
        "        #self.qkv = #...\n",
        "        self.heads = nn.CellList([Head(dim//num_heads) for i in range(num_heads)])\n",
        "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
        "        #self.out = #...\n",
        "\n",
        "    def construct(self, x):\n",
        "        x = x.view(x.shape[0], x.shape[1], self.num_heads, -1)\n",
        "        SA = []\n",
        "        for i in range(self.num_heads):\n",
        "             Sa = nn.Softmax(axis = 2)(self.heads[i](x[:,:,i,:]))\n",
        "             SA.append(Sa)                 \n",
        "        result = mnp.concatenate(SA, axis = 2) #[b, n, dim_embed]\n",
        "        return result"
      ],
      "metadata": {
        "id": "dtri7LYS30pI"
      },
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Cell):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None,\n",
        "                 dropout=0.01):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_features = hidden_features\n",
        "        if hidden_features != None:\n",
        "            self.linear1 = nn.Dense(in_features, hidden_features)\n",
        "            self.linear2 = nn.Dense(hidden_features, out_features)\n",
        "        else:\n",
        "            self.linear1 = nn.Dense(in_features, out_features)\n",
        "        self.GeLU    = nn.GELU()    \n",
        "        self.drop    = nn.Dropout(dropout)\n",
        "\n",
        "        #...\n",
        "\n",
        "    def construct(self, x):\n",
        "        if self.hidden_features != None:\n",
        "            x = self.linear1(x)\n",
        "            x = self.GeLU(x)\n",
        "            x = self.drop(x)\n",
        "            x = self.linear2(x)\n",
        "        else:\n",
        "            x = self.linear1(x)\n",
        "        #...\n",
        "        return x"
      ],
      "metadata": {
        "id": "pQL2FZjEHy_m"
      },
      "execution_count": 402,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Cell):\n",
        "    def __init__(self, dim, n_patches, num_heads=8 ,mlp_ratio=4, drop_rate=0.):\n",
        "        super().__init__()\n",
        "        self.l_norm1    = nn.LayerNorm([n_patches,dim],begin_norm_axis=1,begin_params_axis=1)\n",
        "        self.Attention  = Attention(dim = dim, num_heads = num_heads)\n",
        "        self.l_norm2    = nn.LayerNorm([n_patches,dim],begin_params_axis=1,begin_norm_axis=1)\n",
        "        self.MLP        = ms.Parameter(ms.Tensor(np.random.randn(dim,dim), ms.float32))    \n",
        "        #...\n",
        "\n",
        "    def construct(self, x):\n",
        "\n",
        "        x1 = self.l_norm1(x)\n",
        "        x2 = self.Attention(x1)\n",
        "\n",
        "        x = x + x2\n",
        "        x = mnp.matmul(self.l_norm2(x), self.MLP)\n",
        "        return x"
      ],
      "metadata": {
        "id": "5LqsA6ImH9HS"
      },
      "execution_count": 403,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Cell):\n",
        "    def __init__(self, depth, dim, n_patches, num_heads=8, mlp_ratio=4, drop_rate=0.01):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.CellList([\n",
        "            Block(dim,n_patches, num_heads, mlp_ratio, drop_rate)\n",
        "            for i in range(depth)])\n",
        "\n",
        "    def construct(self, x):\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "sxm4kXFXTKMH"
      },
      "execution_count": 405,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Cell):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_ch=3, num_classes=10,\n",
        "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n",
        "                 drop_rate=0.01):\n",
        "        super().__init__()\n",
        "        self.n_patches = (img_size//patch_size)**2 \n",
        "\n",
        "        self.learn_tok   = ms.Parameter(ms.Tensor(np.random.randn(1,embed_dim), ms.float32))    \n",
        "        self.pos_enc     = ms.Tensor([[pos/(10000**(2*i/embed_dim)) for i in range(embed_dim)] for pos in range(self.n_patches+1)], ms.float32)\n",
        "\n",
        "        self.get_patches = ImgPatches(in_ch,embed_dim,patch_size)\n",
        "\n",
        "        self.drop        = nn.Dropout(drop_rate)\n",
        "        self.transformer = Transformer(depth = depth,\n",
        "                                       dim = embed_dim,\n",
        "                                       n_patches= self.n_patches + 1,\n",
        "                                       num_heads=num_heads,\n",
        "                                       mlp_ratio=mlp_ratio, \n",
        "                                       drop_rate=drop_rate)\n",
        "        self.MLP         = MLP(in_features = embed_dim,\n",
        "                               out_features = num_classes)\n",
        "\n",
        "        #...\n",
        "\n",
        "    def construct(self, x):\n",
        "        #print(type(x),'here23')\n",
        "        x   = self.get_patches(x)\n",
        "        b,n,emb = x.shape\n",
        "        batch_token = self.learn_tok.expand_as(ms.Tensor(np.random.randn(b,1,emb), ms.float32))\n",
        "        x_  = mnp.concatenate([batch_token, x],axis = 1)\n",
        "        batch_pos_enc = self.pos_enc.expand_as(ms.Tensor(np.random.randn(b,self.n_patches+1,emb), ms.float32))\n",
        "\n",
        "\n",
        "        x_ = x_ + batch_pos_enc\n",
        "        x = self.drop(x_)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        x = x[:,0,:]\n",
        "        x = self.MLP(x)\n",
        "        #x = nn.Softmax(axis = -1)(x)\n",
        "\n",
        "        return x.shape"
      ],
      "metadata": {
        "id": "qz56xnN3gQ4M"
      },
      "execution_count": 406,
      "outputs": []
    }
  ]
}